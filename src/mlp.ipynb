{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a92a150-9d91-4f53-9e4d-11955b021bee",
   "metadata": {},
   "source": [
    "# Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76890036-0ba4-4b2d-893a-5098b22d65ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe51091b-8c3d-4a66-82f3-7cc725d14780",
   "metadata": {},
   "source": [
    "# Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "59ffba46-c2df-4fc5-9c35-abde0bb72e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdd218-43e2-4058-b4ff-060accbf6630",
   "metadata": {},
   "source": [
    "# Load the dataset digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "437c34d0-aa0c-4403-8b78-bf0eaffecb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpps\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "digits = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fa6cc-88fa-460d-a2d0-0c078617da05",
   "metadata": {},
   "source": [
    "# Divide the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f190711c-1f3f-41a9-b580-913aaa5b4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### divide to get train and test data with train_test_split() ###\n",
    "X_train, X_test, y_train, y_test = train_test_split((digits.data/255), digits.target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb91b1f-181c-416c-9d3d-64272c820665",
   "metadata": {},
   "source": [
    "# Create and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c404a84-a1d3-4560-9ecd-791b17108079",
   "metadata": {},
   "source": [
    "## Test 1 \n",
    "- One hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 0.1\n",
    "- Batch size = 10\n",
    "- No regularization\n",
    "- No dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7b52f0ae-b457-4532-bcba-f713568da0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### first create the MLP model with the MLP classifier from skit-learn\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # here is the number of nodes and hidden layers\n",
    "    max_iter=60,                # number of trainning steps\n",
    "    solver='sgd',               # optimization algorithm, stochastic gradient descent\n",
    "    verbose=10,                 # quantity of trainning output\n",
    "    learning_rate_init=0.1,     # initial learning_rate, updates network weights\n",
    "    batch_size=10,              # batch size of trainning\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "615720ee-f52e-40b6-8ebe-e62ec63499c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07788106\n",
      "Iteration 2, loss = 1.25970852\n",
      "Iteration 3, loss = 1.19589882\n",
      "Iteration 4, loss = 1.26906618\n",
      "Iteration 5, loss = 1.29825671\n",
      "Iteration 6, loss = 1.44913275\n",
      "Iteration 7, loss = 1.39136293\n",
      "Iteration 8, loss = 1.46936882\n",
      "Iteration 9, loss = 1.52863305\n",
      "Iteration 10, loss = 1.49264735\n",
      "Iteration 11, loss = 1.56743768\n",
      "Iteration 12, loss = 1.50283710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-45 {color: black;background-color: white;}#sk-container-id-45 pre{padding: 0;}#sk-container-id-45 div.sk-toggleable {background-color: white;}#sk-container-id-45 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-45 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-45 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-45 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-45 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-45 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-45 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-45 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-45 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-45 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-45 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-45 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-45 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-45 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-45 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-45 div.sk-item {position: relative;z-index: 1;}#sk-container-id-45 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-45 div.sk-item::before, #sk-container-id-45 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-45 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-45 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-45 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-45 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-45 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-45 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-45 div.sk-label-container {text-align: center;}#sk-container-id-45 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-45 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-45\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, learning_rate_init=0.1, max_iter=60,\n",
       "              random_state=1, solver=&#x27;sgd&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" checked><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=10, learning_rate_init=0.1, max_iter=60,\n",
       "              random_state=1, solver=&#x27;sgd&#x27;, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, learning_rate_init=0.1, max_iter=60,\n",
       "              random_state=1, solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)  # trainning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "362a56c0-9aa7-4688-b357-74b36f8f4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do MLP: 30.99%\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9af00505-7b89-4357-b8da-7cd5334cabd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ed042d5390>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot the losses\n",
    "losses = mlp.loss_curve_\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1e0c4-4a8d-4503-9b75-b5a1e77351cb",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "- Two hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 3.0\n",
    "- Bacth size 10\n",
    "- No regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c1984d9-615e-4ba8-a0cf-8762a82428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,2),# here is the number of hidden layers, in this case 100 neurons in one layer\n",
    "    max_iter=60,               # number of trainning steps\n",
    "    solver='sgd',              # optimization algorithm, gradient descent\n",
    "    verbose=10,                # quantity of trainning output\n",
    "    learning_rate_init=0.1,    # initial learning_rate, updates network weights\n",
    "    batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fa108c8a-2dcf-48cb-b943-50ddc8ce5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27823428\n",
      "Iteration 2, loss = 2.32718385\n",
      "Iteration 3, loss = 2.32638948\n",
      "Iteration 4, loss = 2.32703128\n",
      "Iteration 5, loss = 2.32575757\n",
      "Iteration 6, loss = 2.32470227\n",
      "Iteration 7, loss = 2.32516412\n",
      "Iteration 8, loss = 2.32473966\n",
      "Iteration 9, loss = 2.32389586\n",
      "Iteration 10, loss = 2.32314146\n",
      "Iteration 11, loss = 2.32341222\n",
      "Iteration 12, loss = 2.32400216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-40 {color: black;background-color: white;}#sk-container-id-40 pre{padding: 0;}#sk-container-id-40 div.sk-toggleable {background-color: white;}#sk-container-id-40 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-40 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-40 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-40 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-40 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-40 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-40 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-40 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-40 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-40 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-40 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-40 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-40 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-40 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-40 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-40 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-40 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-40 div.sk-item {position: relative;z-index: 1;}#sk-container-id-40 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-40 div.sk-item::before, #sk-container-id-40 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-40 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-40 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-40 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-40 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-40 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-40 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-40 div.sk-label-container {text-align: center;}#sk-container-id-40 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-40 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-40\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(100, 2),\n",
       "              learning_rate_init=0.1, max_iter=60, solver=&#x27;sgd&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" checked><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(100, 2),\n",
       "              learning_rate_init=0.1, max_iter=60, solver=&#x27;sgd&#x27;, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, hidden_layer_sizes=(100, 2),\n",
       "              learning_rate_init=0.1, max_iter=60, solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "003208fd-3abc-432f-b1d8-13c728ddfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do MLP: 10.24%\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db48fa0-9500-41b6-a62d-7cc33956dac2",
   "metadata": {},
   "source": [
    "## teste 3\n",
    "- One hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 1.0\n",
    "- Batch size = 10\n",
    "- No Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0d80ee19-4ee9-4a59-87b7-dd245955ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,), # here is the number of hidden layers, in this case 100 neurons in one layer\n",
    "    max_iter=60,              # number of trainning steps\n",
    "    solver='sgd',              # optimization algorithm, gradient descent\n",
    "    verbose=10,                # quantity of trainning output\n",
    "    learning_rate_init=1.0,    # initial learning_rate, updates network weights\n",
    "    batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35f0be02-dfb6-471a-a959-d89ef2decd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.06513852\n",
      "Iteration 2, loss = 2.68740842\n",
      "Iteration 3, loss = 2.54137943\n",
      "Iteration 4, loss = 2.48584970\n",
      "Iteration 5, loss = 2.46247931\n",
      "Iteration 6, loss = 2.45620426\n",
      "Iteration 7, loss = 2.45332612\n",
      "Iteration 8, loss = 2.45246046\n",
      "Iteration 9, loss = 2.44767223\n",
      "Iteration 10, loss = 2.45055340\n",
      "Iteration 11, loss = 2.45337352\n",
      "Iteration 12, loss = 2.45165167\n",
      "Iteration 13, loss = 2.45028897\n",
      "Iteration 14, loss = 2.45332619\n",
      "Iteration 15, loss = 2.45180874\n",
      "Iteration 16, loss = 2.45261033\n",
      "Iteration 17, loss = 2.45053471\n",
      "Iteration 18, loss = 2.45264804\n",
      "Iteration 19, loss = 2.45151011\n",
      "Iteration 20, loss = 2.44813040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-41 {color: black;background-color: white;}#sk-container-id-41 pre{padding: 0;}#sk-container-id-41 div.sk-toggleable {background-color: white;}#sk-container-id-41 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-41 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-41 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-41 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-41 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-41 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-41 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-41 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-41 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-41 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-41 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-41 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-41 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-41 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-41 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-41 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-41 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-41 div.sk-item {position: relative;z-index: 1;}#sk-container-id-41 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-41 div.sk-item::before, #sk-container-id-41 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-41 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-41 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-41 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-41 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-41 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-41 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-41 div.sk-label-container {text-align: center;}#sk-container-id-41 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-41 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-41\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, learning_rate_init=1.0, max_iter=30, solver=&#x27;sgd&#x27;,\n",
       "              verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" checked><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=10, learning_rate_init=1.0, max_iter=30, solver=&#x27;sgd&#x27;,\n",
       "              verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, learning_rate_init=1.0, max_iter=30, solver='sgd',\n",
       "              verbose=10)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0da152fe-af65-4676-90eb-fe80ee69bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do MLP: 9.75%\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12d353-8acf-4c60-b737-26753fe66eaf",
   "metadata": {},
   "source": [
    "## teste 3\n",
    "- One hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 0.1\n",
    "- Batch size = 100\n",
    "- No regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f0721dce-7318-48f6-a315-8e8bfd12dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),# here is the number of hidden layers, in this case 100 neurons in one layer\n",
    "    max_iter=60,              # number of trainning steps\n",
    "    solver='sgd',              # optimization algorithm, gradient descent\n",
    "    verbose=10,                # quantity of trainning output\n",
    "    learning_rate_init=0.1,    # initial learning_rate, updates network weights\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c2b3597-5584-41c7-98f9-acf305d4fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.26108772\n",
      "Iteration 2, loss = 0.11512141\n",
      "Iteration 3, loss = 0.08201170\n",
      "Iteration 4, loss = 0.06576728\n",
      "Iteration 5, loss = 0.05431546\n",
      "Iteration 6, loss = 0.04326845\n",
      "Iteration 7, loss = 0.03586087\n",
      "Iteration 8, loss = 0.02805598\n",
      "Iteration 9, loss = 0.02402994\n",
      "Iteration 10, loss = 0.01946936\n",
      "Iteration 11, loss = 0.01475527\n",
      "Iteration 12, loss = 0.01094001\n",
      "Iteration 13, loss = 0.00906730\n",
      "Iteration 14, loss = 0.00774447\n",
      "Iteration 15, loss = 0.00466780\n",
      "Iteration 16, loss = 0.00324205\n",
      "Iteration 17, loss = 0.00226006\n",
      "Iteration 18, loss = 0.00197008\n",
      "Iteration 19, loss = 0.00176256\n",
      "Iteration 20, loss = 0.00162526\n",
      "Iteration 21, loss = 0.00152069\n",
      "Iteration 22, loss = 0.00145785\n",
      "Iteration 23, loss = 0.00140182\n",
      "Iteration 24, loss = 0.00135745\n",
      "Iteration 25, loss = 0.00132796\n",
      "Iteration 26, loss = 0.00129120\n",
      "Iteration 27, loss = 0.00125580\n",
      "Iteration 28, loss = 0.00123440\n",
      "Iteration 29, loss = 0.00120497\n",
      "Iteration 30, loss = 0.00118031\n",
      "Iteration 31, loss = 0.00117139\n",
      "Iteration 32, loss = 0.00114989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-42 {color: black;background-color: white;}#sk-container-id-42 pre{padding: 0;}#sk-container-id-42 div.sk-toggleable {background-color: white;}#sk-container-id-42 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-42 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-42 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-42 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-42 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-42 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-42 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-42 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-42 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-42 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-42 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-42 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-42 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-42 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-42 div.sk-item {position: relative;z-index: 1;}#sk-container-id-42 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-42 div.sk-item::before, #sk-container-id-42 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-42 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-42 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-42 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-42 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-42 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-42 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-42 div.sk-label-container {text-align: center;}#sk-container-id-42 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-42 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-42\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=100, learning_rate_init=0.1, max_iter=60, solver=&#x27;sgd&#x27;,\n",
       "              verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" checked><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=100, learning_rate_init=0.1, max_iter=60, solver=&#x27;sgd&#x27;,\n",
       "              verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=100, learning_rate_init=0.1, max_iter=60, solver='sgd',\n",
       "              verbose=10)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b25e1db3-33d8-40de-aa0b-ca505731dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do MLP: 97.89%\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501bb95-56f1-49af-9459-f5c4c04cbd2a",
   "metadata": {},
   "source": [
    "## teste 4\n",
    "- One hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 0.1\n",
    "- Batch size = 100\n",
    "- Regularization = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4b70f5c6-b451-410c-9dc4-17c2aab98750",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),# here is the number of hidden layers, in this case 100 neurons in one layer\n",
    "    max_iter=60,              # number of trainning steps\n",
    "    solver='sgd',              # optimization algorithm, gradient descent\n",
    "    verbose=10,                # quantity of trainning output\n",
    "    learning_rate_init=0.1,    # initial learning_rate, updates network weights\n",
    "    batch_size=100,\n",
    "    alpha=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cad4801c-5d0b-4eaf-994c-71b58d4fecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25716138\n",
      "Iteration 2, loss = 0.11746517\n",
      "Iteration 3, loss = 0.08424552\n",
      "Iteration 4, loss = 0.06477017\n",
      "Iteration 5, loss = 0.05254433\n",
      "Iteration 6, loss = 0.04326935\n",
      "Iteration 7, loss = 0.03543454\n",
      "Iteration 8, loss = 0.02751893\n",
      "Iteration 9, loss = 0.02437748\n",
      "Iteration 10, loss = 0.01863878\n",
      "Iteration 11, loss = 0.01472417\n",
      "Iteration 12, loss = 0.01140343\n",
      "Iteration 13, loss = 0.00798349\n",
      "Iteration 14, loss = 0.00617038\n",
      "Iteration 15, loss = 0.00401768\n",
      "Iteration 16, loss = 0.00275846\n",
      "Iteration 17, loss = 0.00208654\n",
      "Iteration 18, loss = 0.00180677\n",
      "Iteration 19, loss = 0.00167703\n",
      "Iteration 20, loss = 0.00158431\n",
      "Iteration 21, loss = 0.00148717\n",
      "Iteration 22, loss = 0.00144289\n",
      "Iteration 23, loss = 0.00137982\n",
      "Iteration 24, loss = 0.00133208\n",
      "Iteration 25, loss = 0.00129129\n",
      "Iteration 26, loss = 0.00127040\n",
      "Iteration 27, loss = 0.00124140\n",
      "Iteration 28, loss = 0.00122713\n",
      "Iteration 29, loss = 0.00119876\n",
      "Iteration 30, loss = 0.00117454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acurácia do MLP: 97.90%\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae925e-7cf4-47db-90f6-7aad1b2192bd",
   "metadata": {},
   "source": [
    "## teste 5\n",
    "- One hidden layer\n",
    "- 100 neurons\n",
    "- 60 epochs\n",
    "- Learning rate = 0.1\n",
    "- Batch size = 100\n",
    "- Regularization = 0.0001\n",
    "- Dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "61777001-6bd1-44bd-9815-3415a49c7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://datascience.stackexchange.com/questions/117082/how-can-i-implement-dropout-in-scikit-learn\n",
    "\n",
    "# Creating a custom MLPDropout classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\n",
    "from sklearn.utils import shuffle, gen_batches, check_random_state, _safe_indexing\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "class MLPDropout(MLPClassifier):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation=\"relu\",\n",
    "        *,\n",
    "        solver=\"adam\",\n",
    "        alpha=0.0001,\n",
    "        batch_size=\"auto\",\n",
    "        learning_rate=\"constant\",\n",
    "        learning_rate_init=0.001,\n",
    "        power_t=0.5,\n",
    "        max_iter=200,\n",
    "        shuffle=True,\n",
    "        random_state=None,\n",
    "        tol=1e-4,\n",
    "        verbose=False,\n",
    "        warm_start=False,\n",
    "        momentum=0.9,\n",
    "        nesterovs_momentum=True,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        n_iter_no_change=10,\n",
    "        max_fun=15000,\n",
    "        dropout = None,\n",
    "    ):\n",
    "        '''\n",
    "        Additional Parameters:\n",
    "        ----------\n",
    "        dropout : float in range (0, 1), default=None\n",
    "            Dropout parameter for the model, defines the percentage of nodes\n",
    "            to remove at each layer.\n",
    "            \n",
    "        '''\n",
    "        self.dropout = dropout\n",
    "        super().__init__(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            power_t=power_t,\n",
    "            max_iter=max_iter,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "            tol=tol,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            momentum=momentum,\n",
    "            nesterovs_momentum=nesterovs_momentum,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            epsilon=epsilon,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            max_fun=max_fun,\n",
    "        )\n",
    "    \n",
    "    def _fit_stochastic(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        activations,\n",
    "        deltas,\n",
    "        coef_grads,\n",
    "        intercept_grads,\n",
    "        layer_units,\n",
    "        incremental,\n",
    "    ):\n",
    "        params = self.coefs_ + self.intercepts_\n",
    "        if not incremental or not hasattr(self, \"_optimizer\"):\n",
    "            if self.solver == \"sgd\":\n",
    "                self._optimizer = SGDOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.learning_rate,\n",
    "                    self.momentum,\n",
    "                    self.nesterovs_momentum,\n",
    "                    self.power_t,\n",
    "                )\n",
    "            elif self.solver == \"adam\":\n",
    "                self._optimizer = AdamOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.beta_1,\n",
    "                    self.beta_2,\n",
    "                    self.epsilon,\n",
    "                )\n",
    "\n",
    "        # early_stopping in partial_fit doesn't make sense\n",
    "        early_stopping = self.early_stopping and not incremental\n",
    "        if early_stopping:\n",
    "            # don't stratify in multilabel classification\n",
    "            should_stratify = is_classifier(self) and self.n_outputs_ == 1\n",
    "            stratify = y if should_stratify else None\n",
    "            X, X_val, y, y_val = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                random_state=self._random_state,\n",
    "                test_size=self.validation_fraction,\n",
    "                stratify=stratify,\n",
    "            )\n",
    "            if is_classifier(self):\n",
    "                y_val = self._label_binarizer.inverse_transform(y_val)\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        sample_idx = np.arange(n_samples, dtype=int)\n",
    "\n",
    "        if self.batch_size == \"auto\":\n",
    "            batch_size = min(200, n_samples)\n",
    "        else:\n",
    "            if self.batch_size < 1 or self.batch_size > n_samples:\n",
    "                warnings.warn(\n",
    "                    \"Got `batch_size` less than 1 or larger than \"\n",
    "                    \"sample size. It is going to be clipped\"\n",
    "                )\n",
    "            batch_size = np.clip(self.batch_size, 1, n_samples)\n",
    "\n",
    "        try:\n",
    "            for it in range(self.max_iter):\n",
    "                if self.shuffle:\n",
    "                    # Only shuffle the sample indices instead of X and y to\n",
    "                    # reduce the memory footprint. These indices will be used\n",
    "                    # to slice the X and y.\n",
    "                    sample_idx = shuffle(sample_idx, random_state=self._random_state)\n",
    "\n",
    "                accumulated_loss = 0.0\n",
    "                for batch_slice in gen_batches(n_samples, batch_size):\n",
    "                    if self.shuffle:\n",
    "                        X_batch = _safe_indexing(X, sample_idx[batch_slice])\n",
    "                        y_batch = y[sample_idx[batch_slice]]\n",
    "                    else:\n",
    "                        X_batch = X[batch_slice]\n",
    "                        y_batch = y[batch_slice]\n",
    "                    \n",
    "                    activations[0] = X_batch\n",
    "                    # (DROPOUT ADDITION) layer_units passed forward to help build dropout mask.\n",
    "                    batch_loss, coef_grads, intercept_grads = self._backprop(\n",
    "                        X_batch,\n",
    "                        y_batch,\n",
    "                        activations,\n",
    "                        layer_units,\n",
    "                        deltas,\n",
    "                        coef_grads,\n",
    "                        intercept_grads,\n",
    "                    )\n",
    "                    accumulated_loss += batch_loss * (\n",
    "                        batch_slice.stop - batch_slice.start\n",
    "                    )\n",
    "\n",
    "                    # update weights\n",
    "                    grads = coef_grads + intercept_grads\n",
    "                    self._optimizer.update_params(params, grads)\n",
    "\n",
    "                self.n_iter_ += 1\n",
    "                self.loss_ = accumulated_loss / X.shape[0]\n",
    "\n",
    "                self.t_ += n_samples\n",
    "                self.loss_curve_.append(self.loss_)\n",
    "                if self.verbose:\n",
    "                    print(\"Iteration %d, loss = %.8f\" % (self.n_iter_, self.loss_))\n",
    "\n",
    "                # update no_improvement_count based on training loss or\n",
    "                # validation score according to early_stopping\n",
    "                self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
    "\n",
    "                # for learning rate that needs to be updated at iteration end\n",
    "                self._optimizer.iteration_ends(self.t_)\n",
    "\n",
    "                if self._no_improvement_count > self.n_iter_no_change:\n",
    "                    # not better than last `n_iter_no_change` iterations by tol\n",
    "                    # stop or decrease learning rate\n",
    "                    if early_stopping:\n",
    "                        msg = (\n",
    "                            \"Validation score did not improve more than \"\n",
    "                            \"tol=%f for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "                    else:\n",
    "                        msg = (\n",
    "                            \"Training loss did not improve more than tol=%f\"\n",
    "                            \" for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "\n",
    "                    is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n",
    "                    if is_stopping:\n",
    "                        break\n",
    "                    else:\n",
    "                        self._no_improvement_count = 0\n",
    "\n",
    "                if incremental:\n",
    "                    break\n",
    "\n",
    "                if self.n_iter_ == self.max_iter:\n",
    "                    warnings.warn(\n",
    "                        \"Stochastic Optimizer: Maximum iterations (%d) \"\n",
    "                        \"reached and the optimization hasn't converged yet.\"\n",
    "                        % self.max_iter,\n",
    "                        ConvergenceWarning,\n",
    "                    )\n",
    "        except KeyboardInterrupt:\n",
    "            warnings.warn(\"Training interrupted by user.\")\n",
    "\n",
    "        if early_stopping:\n",
    "            # restore best weights\n",
    "            self.coefs_ = self._best_coefs\n",
    "            self.intercepts_ = self._best_intercepts\n",
    "    \n",
    "    def _backprop(self, X, y, activations, layer_units, deltas, coef_grads, intercept_grads):\n",
    "        \"\"\"Compute the MLP loss function and its corresponding derivatives\n",
    "        with respect to each parameter: weights and bias vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "             \n",
    "        layer_units (DROPOUT ADDITION) : list, length = n_layers\n",
    "            The layer units of the neural net, this is the shape of the\n",
    "            Neural Net model. This is used to build the dropout mask.\n",
    "\n",
    "        deltas : list, length = n_layers - 1\n",
    "            The ith element of the list holds the difference between the\n",
    "            activations of the i + 1 layer and the backpropagated error.\n",
    "            More specifically, deltas are gradients of loss with respect to z\n",
    "            in each layer, where z = wx + b is the value of a particular layer\n",
    "            before passing through the activation function\n",
    "\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            coefficient parameters of the ith layer in an iteration.\n",
    "\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            intercept parameters of the ith layer in an iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        dropout_masks = None\n",
    "        \n",
    "        # Create the Dropout Mask (DROPOUT ADDITION)\n",
    "        if self.dropout != None:\n",
    "            if 0 < self.dropout < 1:\n",
    "                keep_probability = 1 - self.dropout\n",
    "                dropout_masks = [np.ones(layer_units[0])]\n",
    "                \n",
    "                # Create hidden Layer Dropout Masks\n",
    "                for units in layer_units[1:-1]:\n",
    "                    # Create inverted Dropout Mask, check for random_state\n",
    "                    if self.random_state != None:\n",
    "                        layer_mask = (self._random_state.random(units) < keep_probability).astype(int) / keep_probability\n",
    "                    else:\n",
    "                        layer_mask = (np.random.rand(units) < keep_probability).astype(int) / keep_probability\n",
    "                    dropout_masks.append(layer_mask)\n",
    "            else:\n",
    "                raise ValueError('Dropout must be between zero and one. If Dropout=X then, 0 < X < 1.')\n",
    "        \n",
    "        # Forward propagate\n",
    "        # Added dropout_makss to _forward_pass call (DROPOUT ADDITION)\n",
    "        activations = self._forward_pass(activations, dropout_masks)\n",
    "        \n",
    "        # Get loss\n",
    "        loss_func_name = self.loss\n",
    "        if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n",
    "            loss_func_name = \"binary_log_loss\"\n",
    "        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n",
    "        # Add L2 regularization term to loss\n",
    "        values = 0\n",
    "        for s in self.coefs_:\n",
    "            s = s.ravel()\n",
    "            values += np.dot(s, s)\n",
    "        loss += (0.5 * self.alpha) * values / n_samples\n",
    "\n",
    "        # Backward propagate\n",
    "        last = self.n_layers_ - 2\n",
    "\n",
    "        # The calculation of delta[last] here works with following\n",
    "        # combinations of output activation and loss function:\n",
    "        # sigmoid and binary cross entropy, softmax and categorical cross\n",
    "        # entropy, and identity with squared loss\n",
    "        deltas[last] = activations[-1] - y\n",
    "        \n",
    "        # Compute gradient for the last layer\n",
    "        self._compute_loss_grad(\n",
    "            last, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "        )\n",
    "\n",
    "        inplace_derivative = DERIVATIVES[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 2, 0, -1):\n",
    "            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n",
    "            inplace_derivative(activations[i], deltas[i - 1])\n",
    "            \n",
    "            self._compute_loss_grad(\n",
    "                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "            )\n",
    "        \n",
    "        # Apply Dropout Masks to the Parameter Gradients (DROPOUT ADDITION)\n",
    "        if dropout_masks != None:\n",
    "            for layer in range(len(coef_grads)-1):\n",
    "                mask = (~(dropout_masks[layer+1] == 0)).astype(int)\n",
    "                coef_grads[layer] = coef_grads[layer] * mask[None, :]\n",
    "                coef_grads[layer+1] = (coef_grads[layer+1] * mask.reshape(-1, 1))\n",
    "                intercept_grads[layer] = intercept_grads[layer] * mask\n",
    "        \n",
    "        return loss, coef_grads, intercept_grads\n",
    "    \n",
    "    def _forward_pass(self, activations, dropout_masks=None):\n",
    "        \"\"\"Perform a forward pass on the network by computing the values\n",
    "        of the neurons in the hidden layers and the output layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "        dropout_mask : list, length = n_layers - 1\n",
    "            The ith element of the list holds the dropout mask for the ith\n",
    "            layer.\n",
    "        \"\"\"\n",
    "        hidden_activation = ACTIVATIONS[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 1):\n",
    "            activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i])\n",
    "            activations[i + 1] += self.intercepts_[i]\n",
    "            \n",
    "            # For the hidden layers\n",
    "            if (i + 1) != (self.n_layers_ - 1):\n",
    "                hidden_activation(activations[i + 1])\n",
    "            \n",
    "            # Apply Dropout Mask (DROPOUT ADDITION)\n",
    "            if (i + 1) != (self.n_layers_ - 1) and dropout_masks != None:\n",
    "                check1 = activations[i].copy()\n",
    "                activations[i+1] = activations[i+1] * dropout_masks[i+1][None, :]\n",
    "\n",
    "        # For the last layer\n",
    "        output_activation = ACTIVATIONS[self.out_activation_]\n",
    "        output_activation(activations[i + 1])\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "08f53766-f40f-45ae-b9bb-b9f41c29b04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-43 {color: black;background-color: white;}#sk-container-id-43 pre{padding: 0;}#sk-container-id-43 div.sk-toggleable {background-color: white;}#sk-container-id-43 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-43 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-43 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-43 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-43 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-43 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-43 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-43 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-43 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-43 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-43 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-43 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-43 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-43 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-43 div.sk-item {position: relative;z-index: 1;}#sk-container-id-43 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-43 div.sk-item::before, #sk-container-id-43 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-43 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-43 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-43 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-43 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-43 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-43 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-43 div.sk-label-container {text-align: center;}#sk-container-id-43 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-43 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-43\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPDropout(batch_size=100, dropout=0.2, learning_rate_init=0.1, max_iter=60,\n",
       "           solver=&#x27;sgd&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" checked><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPDropout</label><div class=\"sk-toggleable__content\"><pre>MLPDropout(batch_size=100, dropout=0.2, learning_rate_init=0.1, max_iter=60,\n",
       "           solver=&#x27;sgd&#x27;, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPDropout(batch_size=100, dropout=0.2, learning_rate_init=0.1, max_iter=60,\n",
       "           solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPDropout(\n",
    "    hidden_layer_sizes=(100,),# here is the number of hidden layers, in this case 100 neurons in one layer\n",
    "    max_iter=60,              # number of trainning steps\n",
    "    solver='sgd',              # optimization algorithm, gradient descent\n",
    "    verbose=10,                # quantity of trainning output\n",
    "    learning_rate_init=0.1,    # initial learning_rate, updates network weights\n",
    "    batch_size=100,\n",
    "    alpha=0.0001,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eb461b33-dbfd-4ac0-8dfa-76ddc25b3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25968803\n",
      "Iteration 2, loss = 0.11451923\n",
      "Iteration 3, loss = 0.08308565\n",
      "Iteration 4, loss = 0.06198894\n",
      "Iteration 5, loss = 0.05107391\n",
      "Iteration 6, loss = 0.03912944\n",
      "Iteration 7, loss = 0.03435494\n",
      "Iteration 8, loss = 0.02672402\n",
      "Iteration 9, loss = 0.02405690\n",
      "Iteration 10, loss = 0.01712687\n",
      "Iteration 11, loss = 0.01398475\n",
      "Iteration 12, loss = 0.01073988\n",
      "Iteration 13, loss = 0.00833145\n",
      "Iteration 14, loss = 0.00802781\n",
      "Iteration 15, loss = 0.00613875\n",
      "Iteration 16, loss = 0.00367557\n",
      "Iteration 17, loss = 0.00208726\n",
      "Iteration 18, loss = 0.00175754\n",
      "Iteration 19, loss = 0.00157931\n",
      "Iteration 20, loss = 0.00148814\n",
      "Iteration 21, loss = 0.00141628\n",
      "Iteration 22, loss = 0.00136796\n",
      "Iteration 23, loss = 0.00130957\n",
      "Iteration 24, loss = 0.00128841\n",
      "Iteration 25, loss = 0.00124380\n",
      "Iteration 26, loss = 0.00122518\n",
      "Iteration 27, loss = 0.00119511\n",
      "Iteration 28, loss = 0.00117547\n",
      "Iteration 29, loss = 0.00115463\n",
      "Iteration 30, loss = 0.00113586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Acurácia do MLP: 97.83%\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do MLP: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf00da-de27-4ae3-967d-f420947d56dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
